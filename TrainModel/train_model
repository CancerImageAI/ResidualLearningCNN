# -*- coding: utf-8 -*-
"""
Created on Mon Feb 18 11:48:35 2019

@author: Jing Gong
@e-mail: gongjing1990@163.com
@Fudan University Shanghai Cancer Center
"""

import tensorflow as tf
import numpy as np
import glob
import matplotlib.pyplot as plt

 
def get_imgfiles(filepath):
    img_list = []
    label_list = []
    benign_path = filepath+'/non_IAC'
    malignant_path = filepath+'/IAC'
    for img in glob.glob(benign_path+'/*.jpg'):
        ImgName = img.split('\\')[-1]
        img_list.append(benign_path+'/'+ImgName)
        label_list.append('0')
    for img in glob.glob(malignant_path+'/*.jpg'):
        ImgName = img.split('\\')[-1]
        img_list.append(malignant_path+'/'+ImgName)
        label_list.append('1')
    temp = np.array([img_list, label_list])
    temp  = temp.transpose()
    np.random.shuffle(temp)
    image = list(temp[:,0])
    label = list(temp[:,1])
    label = [int(i) for i in label]
    return image, label

def get_batchs(image,label,resize_w, resize_h,batch_size,capacity):
    image = tf.cast(image, tf.string)
    label = tf.cast(label, tf.float64)
    queue = tf.train.slice_input_producer([image, label])
    label = queue[1]
    image_c = tf.read_file(queue[0])
    image = tf.image.decode_jpeg(image_c, channels=3)
    image = tf.image.resize_image_with_crop_or_pad(image,resize_w, resize_h)
    image = tf.image.per_image_standardization(image)
    
    image_batch, label_batch = tf.train.batch([image,label],
                                              batch_size = batch_size,
                                              num_threads = 10,
                                              capacity=capacity)
    image_batch = tf.cast(image_batch, tf.float32)
    label_batch = tf.reshape(label_batch, [batch_size])
    return image_batch, label_batch
    
    
    
def weight_variable(shape):
    initial = tf.truncated_normal(shape,stddev=0.01)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv_2d(x,w,padding='SAME'):
    return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding=padding)

def max_pool_2x2(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')

def avg_pool_nxn(x,n):
    return tf.nn.avg_pool(x,ksize=[1,n,n,1],strides=[1,n,n,1],padding='SAME')

def model(x_image,is_training):#
    

    with tf.name_scope('BN'):
        x_image_bn = tf.layers.batch_normalization(x_image,training=is_training)
        
        
    with tf.name_scope('conv1_1'):
        w1_1 = weight_variable([3,3,3,32])
        b1_1 = bias_variable([32])
        bn1_1 = tf.layers.batch_normalization(conv_2d(x_image_bn,w1_1)+b1_1,training=is_training)
        h_conv1_1 = tf.nn.relu(bn1_1)
        

        
    with tf.name_scope('pool1'):
        h_pool1 = max_pool_2x2(h_conv1_1)
        
    res1_shortcut = h_pool1


    with tf.name_scope('conv2_1'):
        w2_1 = weight_variable([3,3,32,32])
        b2_1 = bias_variable([32])
        bn2_1 = tf.layers.batch_normalization(conv_2d(h_pool1,w2_1)+b2_1,training = is_training)
        h_conv2_1 = tf.nn.relu(bn2_1) 
        

    with tf.name_scope('res2'):
        res2 = tf.nn.relu(tf.add(res1_shortcut,h_conv2_1))
        
    with tf.name_scope('pool2'):
        h_pool2 = max_pool_2x2(res2)
        
    res2_shortcut = h_pool2

        
    with tf.name_scope('conv3_1'):
        w3_1 = weight_variable([3,3,32,32])
        b3_1 = bias_variable([32])
        bn3_1 = tf.layers.batch_normalization(conv_2d(h_pool2,w3_1)+b3_1,training=is_training)
        h_conv3_1 = tf.nn.relu(bn3_1)
      
    with tf.name_scope('res3'):
        res3 = tf.add(res2_shortcut,h_conv3_1)
        
    with tf.name_scope('pool3'):
        h_pool3 = max_pool_2x2(res3)
    
    res3_shortcut = h_pool3  
        
    with tf.name_scope('conv4_0'):
        w4_0 = weight_variable([1,1,32,32])
        b4_0 = bias_variable([32])
        bn4_0 = tf.layers.batch_normalization(conv_2d(h_pool3,w4_0)+b4_0,training=is_training)
        h_conv4_0 = tf.nn.relu(bn4_0) 
        
    with tf.name_scope('conv4_1'):
        w4_1 = weight_variable([3,3,32,32])
        b4_1 = bias_variable([32])
        bn4_1 = tf.layers.batch_normalization(conv_2d(h_pool3,w4_1)+b4_1,training=is_training)
        h_conv4_1 = tf.nn.relu(bn4_1) 
        
    with tf.name_scope('add4'):
        h_add4 = tf.add(h_conv4_0,h_conv4_1)  
        
    with tf.name_scope('res3'):
        res4 = tf.add(res3_shortcut,h_add4)  
        
    with tf.name_scope('pool4'):
        h_pool4 = max_pool_2x2(res4)
                    
    with tf.name_scope('fc1'):
        w5 = weight_variable([4*4*32,16])
        b5 = bias_variable([16])
        h_pool3_flat = tf.reshape(h_pool4,[-1, 4*4*32])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat,w5)+b5)
        
    with tf.name_scope('fc2'):
        w6 = weight_variable([16,2])
        b6 = bias_variable([2])
        y_prob = tf.nn.softmax(tf.add(tf.matmul(h_fc1, w6),b6),name='y_prob')
        
    return y_prob
        
def loss(logits, labels):
    labels = tf.to_int64(labels)
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=labels)
    cost = tf.reduce_mean(cross_entropy)
    return cost

def get_accuracy(predictions, labels):
    labels = tf.to_int64(labels)
    acc = tf.equal(tf.argmax(predictions, 1), labels)
    acc = tf.cast(acc, tf.float32)
    acc = tf.reduce_mean(acc)
    return acc

def training_loss(cost, lr):#
    train_op = tf.train.RMSPropOptimizer(lr,0.9).minimize(cost)
    return train_op

def train():
    tf.reset_default_graph()
    lr = 0.001
    batch_size = 24
    filepath = './IAC_VS_pre/Crop_Img'
    image, label = get_imgfiles(filepath)
    image_batchs, label_batchs = get_batchs(image, label,64,64, batch_size, 100)
    
    x_image = tf.placeholder(tf.float32,[None, 64, 64, 3],name='x_image')
    y_label = tf.placeholder(tf.float32,[None],name='y_label')
    is_training = tf.placeholder(tf.bool,name='is_training')
    y_prob = model(x_image,is_training)
    cost = loss(y_prob,y_label)
    train_op = training_loss(cost, lr)
    acc = get_accuracy(y_prob, y_label)
    fig_loss = np.zeros([5001])
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess,coord)
        try:
            for i in range(5001):
                if  coord.should_stop():
                    break
                image_batchs1, label_batchs1 = sess.run([image_batchs,label_batchs])
                cost1,train_op1 = sess.run([cost,train_op], feed_dict={x_image:image_batchs1,
                                         y_label:label_batchs1,is_training:True})
                
                fig_loss[i] = cost1
                if i % 100 == 0 :
                    train_acc = sess.run(acc, feed_dict={x_image:image_batchs1,
                                         y_label:label_batchs1,is_training:False})
                    print('Step %d, cost: %g' % (i, cost1))
                    print('Step %d, Accuracy: %g' % (i, train_acc))
                if i % 1000 == 0 and i != 0:
                    saver.save(sess,save_path='./model/model.ckpt',global_step=i)

        except tf.errors.OutOfRangeError:
            print('Done!!')
        finally:
            coord.request_stop()
        coord.join(threads=threads)
        plt.plot(np.arange(5001),fig_loss)
        plt.xlabel('iteration')
        plt.ylabel('training loss')
    return fig_loss
        
if __name__ == '__main__':
    fig_loss = train()
     
    
